{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., requires_grad=True)\n",
      "tensor(9., grad_fn=<PowBackward0>)\n",
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor( 3.0 , requires_grad = True )\n",
    "# by default it is false\n",
    "y = x**2\n",
    "print(x)\n",
    "print(y)\n",
    "y.backward()\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., requires_grad=True)\n",
      "tensor(9., grad_fn=<PowBackward0>)\n",
      "tensor(5.1972, grad_fn=<AddBackward0>)\n",
      "tensor(1.6667)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor( 3.0 , requires_grad = True )\n",
    "y = x**2\n",
    "z = torch.log(y) + x\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "\n",
    "# y.backward() => this can not be done because it does not allows you to \n",
    "# calculate intermediate derivatives , only the leaf are allowed.\n",
    "\n",
    "# DAG Graph : leaf => inter => root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., requires_grad=True)\n",
      "tensor(3., requires_grad=True)\n",
      "tensor(36., grad_fn=<AddBackward0>)\n",
      "tensor(-36., grad_fn=<NegBackward0>)\n",
      "tensor(-6.)\n",
      "tensor(-27.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor( 3.0 , requires_grad = True )\n",
    "r = torch.tensor( 3.0 , requires_grad = True )\n",
    "\n",
    "y = x**2 + r**3\n",
    "z = torch.neg(y)\n",
    "\n",
    "print(x)\n",
    "print(r)\n",
    "print(y)\n",
    "print(z)\n",
    "\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "print(r.grad)\n",
    "\n",
    "# Here it consider the rest as constant in case of multivariable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 4., 2.], requires_grad=True)\n",
      "tensor(9.6667, grad_fn=<MeanBackward0>)\n",
      "tensor([2.0000, 2.6667, 1.3333])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor( [3.0,4.0,2.0] , requires_grad = True )\n",
    "y = (x**2).mean()\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "# Here it consider the rest as constant in case of multivariable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Little neural netwrok case \n",
    "\n",
    "1. z = w.x + b\n",
    "\n",
    "2. y_pred = sigmoid(z) = 1 / (1 + e ^ (-z))\n",
    "\n",
    "3. Loss = -( y_tar * log(y_pred) + (1 - y_tar) * log(1 - y_pred) )\n",
    "\n",
    "\n",
    "w = w - (dLoss / dw)\n",
    "\n",
    "b = b - (dLoss / db)\n",
    "\n",
    "So now we need to calculate dLoss / dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_crossentropy(prediction , target):\n",
    "    epsilon = 1e-8 # to avoid log 0\n",
    "    prediction = torch.clamp( prediction ,min =  epsilon ,  max = 1 - epsilon )\n",
    "    loss = -( target * torch.log(prediction) + (1 - target)*torch.log(1 - prediction))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of x : tensor(8.)\n",
      "Value of Ytarget : tensor(1.)\n",
      "Value of w : tensor(1., requires_grad=True)\n",
      "Value of b : tensor(0., requires_grad=True)\n",
      "Value of z : tensor(8., grad_fn=<AddBackward0>)\n",
      "Value of Ypred : tensor(0.9997, grad_fn=<SigmoidBackward0>)\n",
      "Value of loss : tensor(0.0003, grad_fn=<NegBackward0>)\n",
      "Gradient of w : tensor(-0.0027)\n",
      "Gradient of b : tensor(-0.0003)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(8.0)\n",
    "y_target = torch.tensor(1.0)\n",
    "\n",
    "print(\"Value of x :\" , x)\n",
    "print(\"Value of Ytarget :\" , y_target)\n",
    "\n",
    "w = torch.tensor(1.0 , requires_grad=True)\n",
    "b = torch.tensor(0.0 , requires_grad=True)\n",
    "\n",
    "print(\"Value of w :\" , w)\n",
    "print(\"Value of b :\" , b)\n",
    "\n",
    "z = w * x + b\n",
    "y_pred= torch.sigmoid(z)\n",
    "\n",
    "print(\"Value of z :\" , z)\n",
    "print(\"Value of Ypred :\" , y_pred)\n",
    "\n",
    "Loss = binary_crossentropy(y_pred , y_target)\n",
    "\n",
    "print(\"Value of loss :\" , Loss)\n",
    "\n",
    "Loss.backward()\n",
    "\n",
    "print( \"Gradient of w :\", w.grad )\n",
    "print( \"Gradient of b :\" , b.grad )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., requires_grad=True)\n",
      "tensor(9., grad_fn=<PowBackward0>)\n",
      "tensor(6.)\n",
      "tensor(12.)\n",
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor( 3.0 , requires_grad = True )\n",
    "y = x**2\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "y.backward(retain_graph=True)  # Note : bydefault it is False and frees the graph\n",
    "print(x.grad)\n",
    "\n",
    "# You will notice when you again do y.backward()\n",
    "# value of x.grad changes and its add up\n",
    "\n",
    "y.backward(retain_graph=True)\n",
    "print(x.grad)\n",
    "\n",
    "# To avoid this use x.grad.zero_()\n",
    "\n",
    "x.grad.zero_()\n",
    "\n",
    "y.backward(retain_graph=True)\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : \n",
    "\n",
    "1. During training you required gradients to be calculated but during testing we don't.\n",
    "\n",
    "2. There are three options then :\n",
    "\n",
    "    1. requires_grad = False\n",
    "\n",
    "    2. detach()\n",
    "\n",
    "    3. torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., requires_grad=True)\n",
      "tensor(9., grad_fn=<PowBackward0>)\n",
      "tensor(6.)\n",
      "tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor( 3.0 , requires_grad = True )\n",
    "y = x**2\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "x.requires_grad_(False)\n",
    "# x.detach_() can use this also for inplace\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., requires_grad=True)\n",
      "tensor(9., grad_fn=<PowBackward0>)\n",
      "tensor(6.)\n",
      "tensor(3.)\n",
      "tensor(9.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor( 3.0 , requires_grad = True )\n",
    "y = x**2\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "z = x.detach()\n",
    "y = z**2\n",
    "\n",
    "print(z)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., requires_grad=True)\n",
      "tensor(9., grad_fn=<PowBackward0>)\n",
      "tensor(3., requires_grad=True)\n",
      "tensor(9.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor( 3.0 , requires_grad = True )\n",
    "y = x**2\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x**2\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
