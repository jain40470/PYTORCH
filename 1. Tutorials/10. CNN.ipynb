{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is convolution neural network**\n",
    "\n",
    "Convolutional neural networks, also known as convnet, or CNNs, are a\n",
    "special kind of neural network for processing data that has a known\n",
    "grid-like topology like time series data(1D) or images(2D)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN vs ANN** \n",
    "\n",
    "1. Overfitting (as there are more learnable parameters in ann compared to cnn , make it more complex model).\n",
    "\n",
    "2. Loss of imp info like spatial arrangement of pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolution layer and Filters**\n",
    "\n",
    "1. Edge Detection (Convolution Operation) : They do feature extraction like first they extract edges , then shapes etc. \n",
    "\n",
    "2. Each filter is a matrix which has kernel size (eg : 3 => (3,3)) , which is convolve over an area of image (receptive field) , then points at same position first are multiplied then added for a whole.\n",
    "\n",
    "3. For single channel image : (n , n) conv (f,f)  => (n - f + 1 , n - f + 1)\n",
    "\n",
    "4. For multi-channel image : (n , n , c) conv (f,f,c)  => (n - f + 1 , n - f + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding**\n",
    "\n",
    "1. We can notice when convolution is done , we get smaller size .After many layers → The image shrinks too much , information gets lost.\n",
    "\n",
    "2. Without padding, pixels near the edges and edge features get used less for convolution.\n",
    "\n",
    "3. Padding adds a border around the image to resolve this issue.\n",
    "\n",
    "4. There are two types : valid (no padding , image shrinks) and same (padding applied , output size same as input size).\n",
    "\n",
    "5. Output image size : (n + 2p - f + 1, n - 2p + f + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strides**\n",
    "\n",
    "1. Strides are required when high level feature is required.\n",
    "\n",
    "2. It decrease also the computing.\n",
    "\n",
    "3. Output image size : [ (n + 2p - f)/s + 1, (n + 2p - f)/s + 1 ]  (when padding applied) , it considers floor value in division."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem with Convolution**\n",
    "\n",
    "[32 images](228 , 228 , 3) conv (3 , 3 , 3) [100 filters]  => ( 226 , 226, 100) * 32 which is around 634 MB if each value around 4 bytes. (total value * 4 / 1024 ^ 2)\n",
    "\n",
    "1. **Memory issue** :  Above example shows how much memory does convolution takes place.\n",
    "\n",
    "2. **Translation Variance** : Features become location dependant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pooling** \n",
    "\n",
    "1. Pooling down sample you image. (using max pooling , min pooling etc.)\n",
    "\n",
    "2. It helps to reduce the size and Translation invariance (check slides).\n",
    "\n",
    "3. In case of max_pooling it leads in enhanced features.\n",
    "\n",
    "4. There is no need of training. (as there is no training parameters)\n",
    "\n",
    "5. But in some cases where object location is considered , avoid to use pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN Architecture**\n",
    "\n",
    "| Layer               | Parameters                 | Input Shape  | Output Shape | Notes                                 |\n",
    "| ------------------- | -------------------------- | ------------ | ------------ | ------------------------------------- |\n",
    "| **Input**           | —                          | (3, 28, 28)  | (3, 28, 28)  | Image with 3 RGB channels                        |\n",
    "| **Conv Layer 1**    | 32 filters, 3×3, padding=1 | (3, 28, 28)  | (32, 28, 28) | Filters automatically span 3 channels |\n",
    "| **MaxPool 1**       | 2×2, stride=2              | (32, 28, 28) | (32, 14, 14) | Reduces spatial size                  |\n",
    "| **Conv Layer 2**    | 64 filters, 3×3, padding=1 | (32, 14, 14) | (64, 14, 14) | Filters span 32 channels              |\n",
    "| **MaxPool 2**       | 2×2, stride=2              | (64, 14, 14) | (64, 7, 7)   | Still 3D tensor                       |\n",
    "| **Flatten**         | —                          | (64, 7, 7)   | (3136,)      | 64×7×7 = 3136                         |\n",
    "| **Hidden FC Layer** | 64 neurons                 | (3136,)      | (64,)        | Fully connected                       |\n",
    "| **Output Layer**    | 10 neurons                 | (64,)        | (10,)        | Class scores                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BackPropagation in CNN**\n",
    "\n",
    "1. The architecture can be seen as two parts.\n",
    "\n",
    "2. First part include operations like convolution , pooling and flattening.\n",
    "\n",
    "3. Second part can be seen as oprations like matrix muliplication in ANN .\n",
    "\n",
    "4. Check the slides to see how gradients is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12bbed5d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../2. Dataset/fmnist_small.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:, 1:].values/255.0\n",
    "y = df.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain , xtest , ytrain , ytest = train_test_split( x , y , test_size=0.2 , random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "  def __init__(self, features, labels):\n",
    "\n",
    "    self.features = torch.tensor(features, dtype=torch.float32).reshape(-1,1,28,28)  # -1 is used for final dim , it automatically handles that.\n",
    "    self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "  def __len__(self):\n",
    "\n",
    "    return len(self.features)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "\n",
    "    return self.features[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(xtrain,ytrain)\n",
    "test_dataset = CustomDataset(xtest,ytest)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True , pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True , pin_memory=True)\n",
    "# helps to faster copy to gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNN(nn.Module):\n",
    "\n",
    "  def __init__(self, num_channels):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.feature_extraction = nn.Sequential(\n",
    "\n",
    "      nn.Conv2d(num_channels , 32, kernel_size=3, padding='same'),  # input : (1,28,28) , output : (32,28,28)\n",
    "      nn.BatchNorm2d(32), # input : (32,28,28) , output : (32,28,28)\n",
    "      nn.ReLU(), # input : (32,28,28) , output : (32,28,28)\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2), # input : (32,28,28) , output : (32,14,14)\n",
    "\n",
    "      nn.Conv2d(32, 64, kernel_size=3, padding='same'), # input : (32,14,14) , output : (64,14,14)  [64 filter each with (32 channels,3 height,3 width)]\n",
    "      nn.BatchNorm2d(64), # input : (64,14,14) , output : (64,14,14)    [normal applied along channel]\n",
    "      nn.ReLU(), # input : (64,14,14) , output : (64,14,14)\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2) # input : (64,7,7) , output : (64,7,7)\n",
    "\n",
    "    )\n",
    "\n",
    "    self.classifier = nn.Sequential(\n",
    "\n",
    "       nn.Flatten(),\n",
    "\n",
    "       nn.Linear(64*7*7, 128),\n",
    "       nn.ReLU(),\n",
    "       nn.Dropout(p=0.4),\n",
    "\n",
    "       nn.Linear(128, 64),\n",
    "       nn.ReLU(),\n",
    "       nn.Dropout(p=0.4),\n",
    "       \n",
    "       nn.Linear(64, 10)\n",
    "\n",
    "    )\n",
    "\n",
    "  def forward(self, features):\n",
    "\n",
    "    out = self.feature_extraction(features)\n",
    "    out = self.classifier(out)\n",
    "\n",
    "    return out\n",
    "  \n",
    "\n",
    "# In pytorch (batch_size , num of channels , height , width)\n",
    "# conv2d expect no of channels only.\n",
    "\n",
    "# in keras (batch_size , height , width  , num of channels)\n",
    "# there it expect complext image size excluding batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if hasattr(torch,'mps') and torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"MPS is available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MyNN                                     [1, 10]                   --\n",
       "├─Sequential: 1-1                        [1, 64, 7, 7]             --\n",
       "│    └─Conv2d: 2-1                       [1, 32, 28, 28]           320\n",
       "│    └─BatchNorm2d: 2-2                  [1, 32, 28, 28]           64\n",
       "│    └─ReLU: 2-3                         [1, 32, 28, 28]           --\n",
       "│    └─MaxPool2d: 2-4                    [1, 32, 14, 14]           --\n",
       "│    └─Conv2d: 2-5                       [1, 64, 14, 14]           18,496\n",
       "│    └─BatchNorm2d: 2-6                  [1, 64, 14, 14]           128\n",
       "│    └─ReLU: 2-7                         [1, 64, 14, 14]           --\n",
       "│    └─MaxPool2d: 2-8                    [1, 64, 7, 7]             --\n",
       "├─Sequential: 1-2                        [1, 10]                   --\n",
       "│    └─Flatten: 2-9                      [1, 3136]                 --\n",
       "│    └─Linear: 2-10                      [1, 128]                  401,536\n",
       "│    └─ReLU: 2-11                        [1, 128]                  --\n",
       "│    └─Dropout: 2-12                     [1, 128]                  --\n",
       "│    └─Linear: 2-13                      [1, 64]                   8,256\n",
       "│    └─ReLU: 2-14                        [1, 64]                   --\n",
       "│    └─Dropout: 2-15                     [1, 64]                   --\n",
       "│    └─Linear: 2-16                      [1, 10]                   650\n",
       "==========================================================================================\n",
       "Total params: 429,450\n",
       "Trainable params: 429,450\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 4.29\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.60\n",
       "Params size (MB): 1.72\n",
       "Estimated Total Size (MB): 2.32\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_channels = train_dataset[0][0].shape[0]\n",
    "model = MyNN(num_channels)\n",
    "\n",
    "model = model.to(device) # so that weights also move on device\n",
    "\n",
    "num_channels, height, width = train_dataset[0][0].shape\n",
    "# it expects a batch dim and recommended to give tuple\n",
    "summary(model , input_size = (1,num_channels, height, width) , device=device)   # shoudl pass device , else it takes cpu and possibility of runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "learning_rate = 0.1\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akashjain/Desktop/Pytorch/Pyvenv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 , Loss: 1.1967440501848856\n",
      "Epoch: 2 , Loss: 0.7672253000736237\n",
      "Epoch: 3 , Loss: 0.6758850697676341\n",
      "Epoch: 4 , Loss: 0.6118074768781662\n",
      "Epoch: 5 , Loss: 0.5795599378148715\n",
      "Epoch: 6 , Loss: 0.5486063539981842\n",
      "Epoch: 7 , Loss: 0.5056073557337125\n",
      "Epoch: 8 , Loss: 0.4862733202179273\n",
      "Epoch: 9 , Loss: 0.43397352352738383\n",
      "Epoch: 10 , Loss: 0.409811270882686\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "  total_epoch_loss = 0\n",
    "\n",
    "  for batch_features, batch_labels in train_loader:\n",
    "\n",
    "    # move data to gpu \n",
    "    # one way you can also do is in initial only store tensors in data (then train_dataset points to tensor and hence in gpu only)\n",
    "    # .to(device) creates a copy on GPU\n",
    "\n",
    "    batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
    " \n",
    "    outputs = model(batch_features)\n",
    "\n",
    "    loss = criterion(outputs, batch_labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_epoch_loss = total_epoch_loss + loss.item()\n",
    "\n",
    "  avg_loss = total_epoch_loss/len(train_loader)\n",
    "  print(f'Epoch: {epoch + 1} , Loss: {avg_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyNN(\n",
       "  (feature_extraction): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=3136, out_features=128, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Dropout(p=0.4, inplace=False)\n",
       "    (7): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8483333333333334\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "  for batch_features, batch_labels in test_loader:\n",
    "\n",
    "    batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
    "\n",
    "    outputs = model(batch_features)\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1) \n",
    "    # torch.max(input, dim)  ==> maximum along dim 1 i.e along rows\n",
    "    # gives max_values,max_indices #\n",
    "\n",
    "    total = total + batch_labels.shape[0]\n",
    "\n",
    "    correct = correct + (predicted == batch_labels).sum().item()\n",
    "\n",
    "print(correct/total)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
