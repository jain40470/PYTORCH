{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = torchvision.transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = torchvision.transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                             download=True, transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "classes = train_dataset.classes\n",
    "print(classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32]) 6\n"
     ]
    }
   ],
   "source": [
    "img, label = train_dataset[0]\n",
    "print(img.shape, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if hasattr(torch,'mps') and torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"MPS is available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "\n",
    "    def __init__(self,in_channels = 3,num_classes=10,dropout = 0.5):\n",
    "\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),  # (32x32x32)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),  # (32x16x16)\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # (64x16x16)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),  # (64x8x8)\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), # (128x8x8)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)   # (128x4x4)\n",
    "\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*4*4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "\n",
    "\n",
    "    # Hyperparameters to tune\n",
    "\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
    "    \n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['SGD', 'Adam', 'RMSprop'])\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 30)\n",
    "\n",
    "    num_channels = 3\n",
    "    num_classes = 10\n",
    "    \n",
    "    model = SimpleCNN(num_channels,num_classes,dropout_rate).to(device)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "    # Optimizer\n",
    "    if optimizer_name == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        for batch_features, batch_labels in train_loader:\n",
    "\n",
    "            batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(batch_features)\n",
    "\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # how the model layers behave like disable dropout and uses running mean and variance.\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch_features, batch_labels in test_loader:\n",
    "        \n",
    "            batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
    "        \n",
    "            outputs = model(batch_features)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "            total += batch_labels.size()[0]\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akashjain/Desktop/Pytorch/Pyvenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-12-07 20:55:23,864] A new study created in memory with name: no-name-a85aeb58-5b8d-4e7d-88a2-6ad1db5f9707\n",
      "/var/folders/68/g_j6jm596l5dg7k28vc763fh0000gn/T/ipykernel_7071/2270757741.py:6: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/var/folders/68/g_j6jm596l5dg7k28vc763fh0000gn/T/ipykernel_7071/2270757741.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n",
      "/var/folders/68/g_j6jm596l5dg7k28vc763fh0000gn/T/ipykernel_7071/2270757741.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "[I 2025-12-07 21:01:15,402] Trial 0 finished with value: 0.7192 and parameters: {'dropout_rate': 0.3082117547208877, 'weight_decay': 9.319508743623131e-05, 'learning_rate': 0.006240332038931586, 'optimizer': 'SGD', 'batch_size': 64, 'num_epochs': 30}. Best is trial 0 with value: 0.7192.\n",
      "[I 2025-12-07 21:06:34,981] Trial 1 finished with value: 0.823 and parameters: {'dropout_rate': 0.2827641169772, 'weight_decay': 9.369769632110015e-05, 'learning_rate': 0.0002464357490261473, 'optimizer': 'Adam', 'batch_size': 32, 'num_epochs': 26}. Best is trial 1 with value: 0.823.\n",
      "[I 2025-12-07 21:10:34,957] Trial 2 finished with value: 0.8203 and parameters: {'dropout_rate': 0.35530357474312074, 'weight_decay': 9.516026698362807e-05, 'learning_rate': 0.0003184042020037226, 'optimizer': 'Adam', 'batch_size': 64, 'num_epochs': 28}. Best is trial 1 with value: 0.823.\n",
      "[I 2025-12-07 21:13:14,004] Trial 3 finished with value: 0.7562 and parameters: {'dropout_rate': 0.48073225876208775, 'weight_decay': 0.0006518538731566163, 'learning_rate': 0.00013978552809200634, 'optimizer': 'RMSprop', 'batch_size': 32, 'num_epochs': 13}. Best is trial 1 with value: 0.823.\n",
      "[I 2025-12-07 21:16:45,590] Trial 4 finished with value: 0.6374 and parameters: {'dropout_rate': 0.4137726950683262, 'weight_decay': 7.38095676645841e-05, 'learning_rate': 0.008029147070754734, 'optimizer': 'RMSprop', 'batch_size': 64, 'num_epochs': 24}. Best is trial 1 with value: 0.823.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner() \n",
    "# A pruner in Optuna is a smart early-stopping strategy.\n",
    "# It stops training of unpromising trials to save time.\n",
    "\n",
    "study = optuna.create_study(direction='maximize', pruner=pruner)\n",
    "study.optimize(objective, n_trials=5) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'dropout_rate': 0.2827641169772, 'weight_decay': 9.369769632110015e-05, 'learning_rate': 0.0002464357490261473, 'optimizer': 'Adam', 'batch_size': 32, 'num_epochs': 26}\n",
      "Best accuracy: 0.823\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best accuracy:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
