{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Batch Normalization**\n",
    "\n",
    "1. Batch-Normalization (BN) is an algorithmic method which makes \n",
    "the training of Deep Neural Networks (DNN) **faster** and more \n",
    "**stable**.\n",
    "\n",
    "2. It consists of normalizing activation vectors from hidden \n",
    "layers using the **mean and variance** of the current batch. \n",
    "\n",
    "3. This normalization step is applied right before (or right after) the \n",
    "nonlinear function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Where they applied**\n",
    "\n",
    "Applied After Linear Layers and Before Activation Functions.\n",
    "   \n",
    "Example : \n",
    "\n",
    "Case 1 \n",
    "\n",
    "1. Layer : [-5, -2, 0, 3, 6] \n",
    "2. BatchNorm : [-1.2, -0.5, 0, 0.8, 1.5]\n",
    "3. Relu : [0, 0, 0, 0.8, 1.5]\n",
    "\n",
    "Case 2 \n",
    "\n",
    "1. Layer : [-5, -2, 0, 3, 6] \n",
    "2. Relu : [-5, -2, 0, 3, 6] → [0, 0, 0, 3, 6]\n",
    "3. BatchNorm : [-0.5, -0.5, -0.5, 0.2, 1.3]\n",
    "\n",
    "It can be seen that case 1 is better as in case 2 , output lost important information from negative side.But in case 2 , first they are balanced then in relu neg are removed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use BatchNorm** \n",
    "\n",
    "1. In case of unnormalized data , contour plot is like oval and in that case if you keep learning rate high then it overshoots , so we keep lr low which results in slow training.\n",
    "(just check dropout note first slide for how it looks ).\n",
    "\n",
    "2. In case of normalized data , contour plot is like circle , in which you can keep lr high , that makes training faster.\n",
    "\n",
    "3. Due to internal covariate shift , input distribution for furthers layers keep changing , \n",
    " due to thi slater layers constantly need to re-adapt and hence leads to instability and lower training also.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept of Internal Covariate Shift**\n",
    "\n",
    "1. First understand Covariate Shift , it happens when \n",
    "    \n",
    "    1. Input distribution changes\n",
    "\n",
    "    2. But relationship between input → output stays the same\n",
    "\n",
    "2. For example : \n",
    "   \n",
    "    1. Train only on red roses → model learns “rose = red + flower features”\n",
    "    \n",
    "    2. Later see blue/yellow roses → still roses, but input distribution changed.\n",
    "    \n",
    "    3. Model struggles because it never saw these distributions before . So the model has to re-adapt to new types of roses, even though the labeling rule didn’t change.\n",
    "\n",
    "3. Internal Covariate shift is defined as change in distribution of network activations due to change in network parameters during training.\n",
    "\n",
    "4. Internal Covariate Shift (ICS) : Same concept, but happening inside the neural network.\n",
    "   Example : Inside layer 5 of a network , Input distribution keeps changing every batch Because layers 1–4 weights are updating continuously. So deeper layers are constantly receiving different feature distributions, causing : Unstable training ,Slower convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How BatchNorm Works ?** \n",
    "\n",
    "check slides fo example.\n",
    "\n",
    "1. z11 -> z11(N) -> z11(BN) -> g(z11(BN)) -> a11 (check Batch Normalization slides).\n",
    "\n",
    "2. z11 = w1 * (cgpa) + w2 * (iq) + b . Shape: (4 × 2) For each neuron, we normalize across batch (down each column) [4 is batch size] .[mu and sigma i.e mean and std devn  is not learnable]  [mean = 0 , std devn = 1 , (std devn)^2 = variance = 1]\n",
    "\n",
    "3. z11(BN) = gamma * (z11(N)) + beta * (z11(N)) , then further g(z11(NB)) is activation , \n",
    "where gamman and beta are learnable paramaters.\n",
    "\n",
    "4. While normalizing z11 , we add epsilon in denominator so that in case sigma get 0 , it handles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch Norm during test**\n",
    "\n",
    "1. During testing we are providing the data in batches , but in test we provide a single row , then in that case how is z11 normalized ? (as we required mean and std devn and for single row how they can be calculated).\n",
    "\n",
    "2. For that , we calculated mean and variance using exponential weighted meaning average.For each neuron seperately.\n",
    "\n",
    "3. check for expression on internet its like [val_store = alpha * val_store + (1-alpha)*current_caluculated] and for std devn conisder square. [where alpha is momentum , a hyper parameter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages**\n",
    "\n",
    "1. Make training stable.\n",
    "\n",
    "2. Make training faster , as you can choose high lr.\n",
    "\n",
    "3. Act as regularizer in some sense . (not too much extent like dropout).[like value of mean and std devn depends on batch itself (if batch changes) , then lead to changes in activation and that can introduce a little randomness or some noise which leads to little decrase in overfitting].\n",
    "\n",
    "4. Not to worry about Weight initialization , as its impact reduce because now normalization happens so it converges (check slides).  [cost function stretched without using it and now it takes time to reach optimal solution , but if you used it , then it is uniform , then you can reach optimal solution in a better way.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DropOuts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is DropOut and how it works**\n",
    "\n",
    "1. Dropout is a technique used to reduce overfitting by **randomly turns off p% neuron**s in the hidden layer during each forward pass. It encourages neurons to learn independent useful representations\n",
    "\n",
    "2. It is applied to the hidden layers and applied after the ReLU activation function.\n",
    "\n",
    "3. This has a **regularization** effect because it prevents a neural network from relying too heavily on specific neurons during training.\n",
    "\n",
    "4. If overfitting is there increase the value of p , if underfitting is there decrease the valeu of p. [it is a tip to first apply dopout to last layer then check]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it works in testing** \n",
    "\n",
    "1. No dropout applied.\n",
    "\n",
    "2. All neurons active\n",
    "\n",
    "3. Their outputs (weights) are scaled (typically multiplied by 1−p) so that the expected activation matches training. (see the notes pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DrawBacks**\n",
    "\n",
    "1. It delays the convergence (means go to right weights and bias) because neurons randomly drop, gradients become noisy ,so model takes longer to learn patterns. [Means training become a little slow]\n",
    "\n",
    "2. The value of cost function changes.(as in every epoch , some nodes are not consider).It faces some issue in debugging and  the calculation of gradients become a little difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Regularization**\n",
    "\n",
    "1. Regularization is applied to the weights of the model to penalize large values and\n",
    "encourage smaller, more generalizable weights.\n",
    "\n",
    "2. Adds a penalty term λ∑wi2 to the loss function in L2 regularization.\n",
    "\n",
    "3. In weight decay, directly modifies the gradient update rule to include λwi, effectively\n",
    "shrinking weights during training.\n",
    "\n",
    "4. Encourages the network to distribute learning across multiple parameters, avoiding\n",
    "reliance on a few large weights.  [Just remeber large weights leads to memoize noise and unstability , for example : 100*(51 - 50) : 100 , where as if 1 * (51-50) then it will be 1 , if its a 100 diff then model try to memoize it i.e Model becomes too sensitive and memorizes tiny noise in data.]\n",
    "\n",
    "5. Regularization is typically applied only to weights, not biases, as biases don't directly\n",
    "affect model complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
