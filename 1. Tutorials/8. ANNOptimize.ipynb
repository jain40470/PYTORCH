{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Batch Normalization**\n",
    "\n",
    "1. Batch-Normalization (BN) is an algorithmic method which makes \n",
    "the training of Deep Neural Networks (DNN) **faster** and more \n",
    "**stable**.\n",
    "\n",
    "2. It consists of normalizing activation vectors from hidden \n",
    "layers using the **mean and variance** of the current batch. \n",
    "\n",
    "3. This normalization step is applied right before (or right after) the \n",
    "nonlinear function.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use BatchNorm** \n",
    "\n",
    "1. In case of unnormalized data , contour plot is like oval and in that case if you keep learning rate high then it overshoots , so we keep lr low which results in slow training.\n",
    "(just check dropout note first slide for how it looks ).\n",
    "\n",
    "2. In case of normalized data , contour plot is like circle , in which you can keep lr high , that makes training faster.\n",
    "\n",
    "3. Due to internal covariate shift , input distribution for furthers layers keep changing , \n",
    " due to thi slater layers constantly need to re-adapt and hence leads to instability and lower training also.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept of Internal Covariate Shift**\n",
    "\n",
    "1. First understand Covariate Shift , it happens when \n",
    "    \n",
    "    1. Input distribution changes\n",
    "\n",
    "    2. But relationship between input → output stays the same\n",
    "\n",
    "2. For example : \n",
    "   \n",
    "    1. Train only on red roses → model learns “rose = red + flower features”\n",
    "    \n",
    "    2. Later see blue/yellow roses → still roses, but input distribution changed.\n",
    "    \n",
    "    3. Model struggles because it never saw these distributions before . So the model has to re-adapt to new types of roses, even though the labeling rule didn’t change.\n",
    "\n",
    "3. Internal Covariate shift is defined as change in distribution of network activations due to change in network parameters during training.\n",
    "\n",
    "4. Internal Covariate Shift (ICS) : Same concept, but happening inside the neural network.\n",
    "   Example : Inside layer 5 of a network , Input distribution keeps changing every batch Because layers 1–4 weights are updating continuously. So deeper layers are constantly receiving different feature distributions, causing : Unstable training ,Slower convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How BatchNorm Works ?** \n",
    "\n",
    "check slides fo example.\n",
    "\n",
    "1. z11 -> z11(N) -> z11(BN) -> g(z11(BN)) -> a11 (check Batch Normalization slides).\n",
    "\n",
    "2. z11 = w1 * (cgpa) + w2 * (iq) + b . Shape: (4 × 2) For each neuron, we normalize across batch (down each column) [4 is batch size] .[mu and sigma i.e mean and std devn  is not learnable]  [mean = 0 , std devn = 1 , (std devn)^2 = variance = 1]\n",
    "\n",
    "3. z11(BN) = gamma * (z11(N)) + beta * (z11(N)) , then further g(z11(NB)) is activation , \n",
    "where gamman and beta are learnable paramaters.\n",
    "\n",
    "4. While normalizing z11 , we add epsilon in denominator so that in case sigma get 0 , it handles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch Norm during test**\n",
    "\n",
    "1. During testing we are providing the data in batches , but in test we provide a single row , then in that case how is z11 normalized ? (as we required mean and std devn and for single row how they can be calculated).\n",
    "\n",
    "2. For that , we calculated mean and variance using exponential weighted meaning average.For each neuron seperately.\n",
    "\n",
    "3. check for expression on internet its like [val_store = alpha * val_store + (1-alpha)*current_caluculated] and for std devn conisder square. [where alpha is momentum , a hyper parameter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages**\n",
    "\n",
    "1. Make training stable.\n",
    "\n",
    "2. Make training faster , as you can choose high lr.\n",
    "\n",
    "3. Act as regularizer in some sense . (not too much extent like dropout).[like value of mean and std devn depends on batch itself (if batch changes) , then lead to changes in activation and that can introduce a little randomness or some noise which leads to little decrase in overfitting].\n",
    "\n",
    "4. Not to worry about Weight initialization , as its impact reduce because now normalization happens so it converges (check slides).  [cost function stretched without using it and now it takes time to reach optimal solution , but if you used it , then it is uniform , then you can reach optimal solution in a better way.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DropOuts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is DropOut and how it works**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it works in testing** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DrawBacks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Regularization**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
